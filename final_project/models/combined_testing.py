# -*- coding: utf-8 -*-
"""Combined_Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R80u2tD2MZTonf9CnMo_dLav5V47M-oz
"""

# Connect to your google drive
import os

import torch
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import cv2
import torch.nn as nn
import time 
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit


class F110_YOLO(torch.nn.Module):
    def __init__(self):
        super(F110_YOLO, self).__init__()
        # TODO: Change the channel depth of each layer
        self.conv1 = nn.Conv2d(3, 4, kernel_size = 4, padding = 1, stride = 2)
        self.batchnorm1 = nn.BatchNorm2d(4)
        self.relu1 = nn.ReLU(inplace = True)

        self.conv2 = nn.Conv2d(4, 48, kernel_size = 4, padding = 1, stride = 2)
        self.batchnorm2 = nn.BatchNorm2d(48)
        self.relu2 = nn.ReLU(inplace = True)

        self.conv3 = nn.Conv2d(48, 162, kernel_size = 4, padding = 1, stride = 2)
        self.batchnorm3 = nn.BatchNorm2d(162)
        self.relu3 = nn.ReLU(inplace = True)
        
        self.conv4 = nn.Conv2d(162, 256, kernel_size = 4, padding = 1, stride = 2)
        self.batchnorm4 = nn.BatchNorm2d(256)
        self.relu4 = nn.ReLU(inplace = True)

        self.conv5 = nn.Conv2d(256, 512, kernel_size = 4, padding = 1, stride = 2)
        self.batchnorm5 = nn.BatchNorm2d(512)
        self.relu5 = nn.ReLU(inplace = True)

        self.conv6 = nn.Conv2d(512,256, kernel_size = 3, padding = 1, stride = 1)
        self.batchnorm6 = nn.BatchNorm2d(256)
        self.relu6 = nn.ReLU(inplace = True)

        self.conv7 = nn.ConvTranspose2d(256, 64, kernel_size = 3, padding = 1, stride = 1)
        self.batchnorm7 = nn.BatchNorm2d(64)
        self.relu7 = nn.ReLU(inplace = True)

        self.conv8 = nn.ConvTranspose2d(64, 6, kernel_size = 3, padding = 1, stride = 1)
        self.batchnorm8 = nn.BatchNorm2d(6)
        self.relu8 = nn.ReLU(inplace = True)

        self.conv9 = nn.Conv2d(6, 5, kernel_size = 1, padding = 0, stride = 1)
        self.relu9 = nn.ReLU()
    
    def forward(self, x):
        debug = 0 # change this to 1 if you want to check network dimensions
        if debug == 1: print(0, x.shape)
        x = torch.relu(self.batchnorm1(self.conv1(x)))
        if debug == 1: print(1, x.shape)
        x = torch.relu(self.batchnorm2(self.conv2(x)))
        if debug == 1: print(2, x.shape)
        x = torch.relu(self.batchnorm3(self.conv3(x)))
        if debug == 1: print(3, x.shape)
        x = torch.relu(self.batchnorm4(self.conv4(x)))
        if debug == 1: print(4, x.shape)
        x = torch.relu(self.batchnorm5(self.conv5(x)))
        if debug == 1: print(5, x.shape)
        x = torch.relu(self.batchnorm6(self.conv6(x)))
        if debug == 1: print(6, x.shape)
        x = torch.relu(self.batchnorm7(self.conv7(x)))
        if debug == 1: print(7, x.shape)
        x = torch.relu(self.batchnorm8(self.conv8(x)))
        if debug == 1: print(8, x.shape)
        x = self.conv9(x)
        if debug == 1: print(9, x.shape)
        x = torch.cat([x[:, 0:3, :, :], torch.sigmoid(x[:, 3:5, :, :])], dim=1)

        return x

    def get_loss(self, result, truth, lambda_coord = 5, lambda_noobj = 1):
        x_loss = (result[:, 1, :, :] - truth[:, 1, :, :]) ** 2
        y_loss = (result[:, 2, :, :] - truth[:, 2, :, :]) ** 2
        w_loss = (torch.sqrt(result[:, 3, :, :]) - torch.sqrt(truth[:, 3, :, :])) ** 2
        h_loss = (torch.sqrt(result[:, 4, :, :]) - torch.sqrt(truth[:, 4, :, :])) ** 2
        class_loss_obj = truth[:, 0, :, :] * (truth[:, 0, :, :] - result[:, 0, :, :]) ** 2
        class_loss_noobj = (1 - truth[:, 0, :, :]) * lambda_noobj * (truth[:, 0, :, :] - result[:, 0, :, :]) ** 2

        total_loss = torch.sum(lambda_coord * truth[:, 0, :, :] * (x_loss + y_loss + w_loss + h_loss) + class_loss_obj + class_loss_noobj)
        
        return total_loss

## Converting label to bounding boxes

def label_to_box_xyxy(result, threshold = 0.9):
    validation_result = []
    result_prob = []
    for ind_row in range(final_dim[0]):
        for ind_col in range(final_dim[1]):
            grid_info = grid_cell(ind_col, ind_row)
            validation_result_cell = []
            if result[0, ind_row, ind_col] >= threshold:
                c_x = grid_info[0] + anchor_size[1]/2 + result[1, ind_row, ind_col]
                c_y = grid_info[1] + anchor_size[0]/2 + result[2, ind_row, ind_col]
                w = result[3, ind_row, ind_col] * input_dim[1]
                h = result[4, ind_row, ind_col] * input_dim[0]
                x1, y1, x2, y2 = bbox_convert(c_x, c_y, w, h)
                x1 = np.clip(x1, 0, input_dim[1])
                x2 = np.clip(x2, 0, input_dim[1])
                y1 = np.clip(y1, 0, input_dim[0])
                y2 = np.clip(y2, 0, input_dim[0])
                validation_result_cell.append(x1)
                validation_result_cell.append(y1)
                validation_result_cell.append(x2)
                validation_result_cell.append(y2)
                result_prob.append(result[0, ind_row, ind_col])
                validation_result.append(validation_result_cell)
    validation_result = np.array(validation_result)
    result_prob = np.array(result_prob)
    return validation_result, result_prob


def voting_suppression(result_box, iou_threshold = 0.5):
    votes = np.zeros(result_box.shape[0])
    for ind, box in enumerate(result_box):
        for box_validation in result_box:
            if IoU(box_validation, box) > iou_threshold:
                votes[ind] += 1
    return (-votes).argsort()

# convert feature map coord to image coord
def grid_cell(cell_indx, cell_indy):
    stride_0 = anchor_size[1]
    stride_1 = anchor_size[0]
    return np.array([cell_indx * stride_0, cell_indy * stride_1, cell_indx * stride_0 + stride_0, cell_indy * stride_1 + stride_1])

# convert from [c_x, c_y, w, h] to [x_l, y_l, x_r, y_r]
def bbox_convert(c_x, c_y, w, h):
    return [c_x - w/2, c_y - h/2, c_x + w/2, c_y + h/2]

# convert from [x_l, y_l, x_r, x_r] to [c_x, c_y, w, h]
def bbox_convert_r(x_l, y_l, x_r, y_r):
    return [x_l/2 + x_r/2, y_l/2 + y_r/2, x_r - x_l, y_r - y_l]

# calculating IoU
def IoU(a, b):
    # referring to IoU algorithm in slides
    inter_w = max(0, min(a[2], b[2]) - max(a[0], b[0]))
    inter_h = max(0, min(a[3], b[3]) - max(a[1], b[1]))
    inter_ab = inter_w * inter_h
    area_a = (a[3] - a[1]) * (a[2] - a[0])
    area_b = (b[3] - b[1]) * (b[2] - b[0])
    union_ab = area_a + area_b - inter_ab
    return inter_ab / union_ab

def assign_label(label):
    label_gt = np.zeros((5, final_dim[0], final_dim[1]))
    IoU_threshold = 0.01
    IoU_max = 0
    IoU_max_ind = [0, 0]

def DisplayLabel(img, bboxs):
    # image = np.transpose(image.copy(), (1, 2, 0))
    # fig, ax = plt.subplots(1, figsize=(6, 8))
    image = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)
    fig, ax = plt.subplots(1)
    edgecolor = [1, 0, 0]
    if len(bboxs) == 1:
        bbox = bboxs[0]
        ax.add_patch(patches.Rectangle((bbox[0] - bbox[2]/2, bbox[1] - bbox[3]/2), bbox[2], bbox[3], linewidth=1, edgecolor=edgecolor, facecolor='none'))
    elif len(bboxs) > 1:
        for bbox in bboxs:
            ax.add_patch(patches.Rectangle((bbox[0] - bbox[2]/2, bbox[1] - bbox[3]/2), bbox[2], bbox[3], linewidth=1, edgecolor=edgecolor, facecolor='none'))
    ax.imshow(image)
    
    plt.plot(bboxs[0][0],bboxs[0][1] + (bboxs[0][3]/2), marker="o", markersize=20, markeredgecolor="red", markerfacecolor="green")
    
    plt.show()

object_in_class = 0
truth_in_class = 0
voting_iou_threshold = 0.5
confi_threshold = 0.4
final_dim = [5, 10]
input_dim = [180, 320]
anchor_size = [(input_dim[0] / final_dim[0]), (input_dim[1] / final_dim[1])]

#OpenCV camera stuff
cap = cv2.VideoCapture("v4l2src device=/dev/video2 extra-controls=\"c,exposure_auto=3\" ! video/x-raw, width=960, height=540 ! videoconvert ! video/x-raw,format=BGR ! appsink")
time_old = time.time()
if cap.isOpened():
    cv2.namedWindow("demo", cv2.WINDOW_AUTOSIZE)
    time_new = time.time()
    ret_val, img = cap.read()
else:
    print("Camera open failed")
cv2.destroyAllWindows()

image = img 
image_ = img
# fig, ax = plt.subplots(1, figsize=(6, 8))
# image = np.transpose(image.copy(), (1, 2, 0))
fig, ax = plt.subplots(1)
ax.imshow(image)
plt.show()


image_ = image_/ 255.0
image_original = cv2.resize(image_, (input_dim[1], input_dim[0]))
image_t = np.transpose(image_original,(2,0,1))
image_t = image_t.reshape(1,image_t.shape[0],image_t.shape[1],image_t.shape[2])
#image_t = image_original.reshape(1,3,image_original.shape[0],image_original.shape[1])

image_tensor = torch.from_numpy(image_t).type('torch.FloatTensor')


#Load and evaluate NN model 
TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)
runtime = trt.Runtime(TRT_LOGGER)
with open('./group4_engine.engine', 'rb') as f:
    engine_bytes = f.read()
engine = runtime.deserialize_cuda_engine(engine_bytes)
context = engine.create_execution_context()
for binding in engine:
    if engine.binding_is_input(binding):  # we expect only one input
        input_shape = engine.get_binding_shape(binding)
        input_size = trt.volume(input_shape) * engine.max_batch_size * np.dtype(np.float32).itemsize  # in bytes
        device_input = cuda.mem_alloc(input_size)
    else:  # and one output
        output_shape = engine.get_binding_shape(binding)
        # create page-locked memory buffers (i.e. won't be swapped to disk)
        host_output = cuda.pagelocked_empty(trt.volume(output_shape) * engine.max_batch_size, dtype=np.float32)
        device_output = cuda.mem_alloc(host_output.nbytes)

# Create a stream in which to copy inputs/outputs and run inference.
stream = cuda.Stream()

# preprocess input data
print(f"Input shape {image_tensor.shape}")
host_input = np.array(image_tensor, dtype=np.float32, order='C')
cuda.memcpy_htod_async(device_input, host_input, stream)

# run inference
context.execute_async(bindings=[int(device_input), int(device_output)], stream_handle=stream.handle)
cuda.memcpy_dtoh_async(host_output, device_output, stream)
stream.synchronize()

# postprocess results
result = np.array(host_output)
print(result.shape)
result = result.reshape(5, 5, 10)

# result = model(image_tensor)
# result = result.detach().cpu().numpy()
bboxs, result_prob = label_to_box_xyxy(result, confi_threshold)
vote_rank = voting_suppression(bboxs, voting_iou_threshold)
bbox = bboxs[vote_rank[0]]
[c_x, c_y, w, h] = bbox_convert_r(bbox[0], bbox[1], bbox[2], bbox[3])
bboxs_2 = np.array([[c_x, c_y, w, h]])
DisplayLabel(np.transpose(image_tensor[0].numpy(), (1, 2, 0)), bboxs_2)

#Lane Detections
blur_image = cv2.GaussianBlur(img, (5,5), 2)

'''
plt.figure(1)
plt.imshow(blur_image, animated=True)
plt.draw()
plt.pause(0.0001)
plt.clf()'''

hsv = cv2.cvtColor(blur_image, cv2.COLOR_BGR2HSV)

'''
plt.figure(2)
plt.imshow(hsv,animated=True)
plt.draw()
plt.pause(0.0001)
plt.clf()'''

mask = cv2.inRange(hsv, np.array([20, 70, 70]), np.array([40, 255, 255]))
# Bitwise-AND mask and original image
res = cv2.bitwise_and(hsv,hsv, mask= mask)

'''
plt.figure(3)
plt.imshow(res,animated=True)
plt.draw()
plt.pause(0.0001)
plt.clf()'''


contours, hierarchy = cv2.findContours(image=mask, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)
# draw contours on the original image
image_copy = img.copy()
cv2.drawContours(image=image_copy, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=5, lineType=cv2.LINE_AA)


plt.imshow(image_copy)
cv2.waitKey(0)
cv2.imwrite('lane_detection.jpg', image_copy)
cv2.destroyAllWindows()

image_final = image_copy/ 255.0
image_orig = cv2.resize(image_final, (input_dim[1], input_dim[0]))
image_f = np.transpose(image_orig,(2,0,1))
image_f = image_f.reshape(1,image_f.shape[0],image_f.shape[1],image_f.shape[2])
image_f_tensor= torch.from_numpy(image_f).type('torch.FloatTensor')

DisplayLabel(np.transpose(image_f_tensor[0].numpy(), (1, 2, 0)), bboxs_2)

#Calculate Distance
x_position = bboxs[0][0] * (image_t.shape[2]/image.shape[0])
y_position = (bboxs[0][1] + (bboxs[0][3]/2)) * (image_t.shape[3]/image.shape[1])

class distances:
    
    def get_car_coords(self, x_in, y_in):
        x_car = (self.fy / (y_in-self.y0)) * (self.z_car + self.h_mount)
        y_car = (x_in - self.x0) * (x_car / self.fx)
        return x_car, y_car
        
    def __init__(self):  
                
        mtx = np.load("calibration_maxtix.npy")
        dist = np.load("calibration_dist.npy")
        

        h,  w = image.shape[:2]
        newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))
        
        # undistort
        dst = cv2.undistort(image, mtx, dist, None, newcameramtx)
        # crop the image
      
        #cv2.imshow(str(i), dst[y:y+h, x:x+w])
        plt.figure(1)
        plt.imshow(image)#dst[y:y+h, x:x+w])
        #Finding Mount Height using provided distance image
        self.y0 = 0
        self.x0 = 0
        self.x_car =0.4#Distance in meters of car from cone
        self.z_car = 0#Height of object above the ground
        self.fx = mtx[0,0]
        self.fy = mtx[1,1]
        y = 496.95
        self.h_mount = (y - self.y0) * (self.x_car / self.fy) - self.z_car
        
        print("Mount Height (cm) =",self.h_mount * 100)
        x_val,y_val = self.get_car_coords(x_position,y_position)
        #print("x_distance(cm) =",x_val*100)
        print("y distance(cm)",y_val*100)
        
     
distances()
